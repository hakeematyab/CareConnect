{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4t4EcWftewG"
   },
   "source": [
    "# Meta Llama-3 Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "03S9I0RNY44i"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "from transformers.integrations import WandbCallback\n",
    "from transformers import (\n",
    "        AutoTokenizer, pipeline,\n",
    "        AutoModelForCausalLM,\n",
    "        DataCollatorWithPadding,\n",
    "        DataCollatorForSeq2Seq,\n",
    "        AutoModelForSpeechSeq2Seq,\n",
    "        BartForConditionalGeneration,\n",
    "        TrainingArguments,\n",
    "        Seq2SeqTrainingArguments,\n",
    "        Trainer,\n",
    "        BitsAndBytesConfig,\n",
    "        EarlyStoppingCallback,\n",
    "        ProgressCallback\n",
    ")\n",
    "from huggingface_hub import login\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"CareConnect\",name=\"Fine-tuning with context(Accurate 50% of times)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UA-cstFQa4s2"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438,
     "referenced_widgets": [
      "77adf02bed424f7c8d6669737cc10f01",
      "fb7ee9fcc49f4e3aba9cf70dc6286dac",
      "c75b6da952274fad9986dd35b6ea9d76",
      "e58a3f6d2d3f4fc9bebab66dd9f3ad54",
      "81c4ef66406f4069bf9ce257eef5ee4c",
      "c188fdfcb06d4446a36dad010daabf87",
      "b7924875913849c9b9136a205aa343e1",
      "3ae9d599f2624d25bbb4eb6b790733a2",
      "b918c1fbfa01408a82c1f766b1852ce2",
      "ef952aea4c8e4c77ad8e2b55cb8e4d53",
      "671f9c57f8f74cdaa1ad60f30339c242",
      "93c1a674c8344646a8d2880bc9c077c4",
      "59c043d4318c43e590cee4159dd647b0",
      "a0c0b7a972d44b75a049068d80f4d1c5",
      "f4ed0d1795314f13b3b6c8ffabb50012",
      "27740b38e5f84482bf3c1d10e1db0f61",
      "373b15aefa2940f390acb5b58ce2af38",
      "b732b04dda66486195ad6ed9d342bd39",
      "2fc4bd7203f84de8a0052cea77323aa8",
      "a9dc03a2e6db4c85b63966e5b32de9ee",
      "6e3f2c2d3dfa46788e534e1059463d93",
      "fb70edb9e151433081351b6f4848009d",
      "8d7882b80c32447da82dd1f2931c3329",
      "82aebf4c3ed94c408b757cea9b4102dc",
      "ca2e1ee4b3a243bfa55cb767b29d65a7",
      "e95f0323999d44a4970ceab2c5b40158",
      "d40948c5fb074d2abc2547977f403328",
      "55e30e418af24c7c8a9d8ce39793744f",
      "7c0398a8a1484ecf8850111ae87b3e66",
      "084c441f92c44ee89e0242769ad2fb92",
      "1930bd12cee047d8b8b86729c3508141",
      "28aa897984924c4e8a29418f174f112c",
      "8074f75a450942f3a7b7dea4c0305fa9",
      "0c4cd5e1415c4598973f7d246193e407",
      "4be6fe8f4f344ac19f97367adec8ecd4",
      "3ad567c574914991b7120bb7882f982c",
      "bca2c564144348e58f4bba5304782b76",
      "4380cb7679f14557ad002c728674863c",
      "a7e1657a063c4c40836a5e82be54d6de",
      "8a4ad37a04e44645828953f88d0d367a",
      "30247976674c4016af43217884cc695e",
      "a01a0cb6265e469088c8ca343e3c26d9",
      "4fb7ae3ac78c4bd18fda81c0e56267ec",
      "8bdbb9dfd1c94378a46a7dd881f842fa",
      "64674231f9cc4929999f1342b3961a2e",
      "7c6bff4be8d24d93bdf404be9b0ba4cf",
      "8bc9ac4ab35943d7811ce127e643339d",
      "d520d8d84aa444ab9b1122dd2c375dfd",
      "d69d7c4d898d49f2b7a049ee5f7dd934",
      "2d344fe00f464be18ade51ed8d1f0c76",
      "2e6cb0f337cb49bf89e5d6a3cabe1517",
      "b66b8ccec33c49ba84dbf31d64f8ce2f",
      "e036a6e15aab42339ae4a26ad97cd95c",
      "de982f4f6557419d9924b82ccf810ba7",
      "b4c175b79bc84472b3de3c612c645e78",
      "5b59e926ee1c4c0daf179f7fbc297414",
      "b34a0ce3ea35493eb223be3a50598f7a",
      "ea261420d135410ca8a44c355865863b",
      "c4050d40e22248cfaf44d5ed2ca5732f",
      "fabcecf935a845fbbb06f75014e5a1c6",
      "40db3b3a91e3497ca6d4f11e88585a96",
      "9c9482efcb064a11951c4d8529162a84",
      "f9f34a1e8daa45e69ed15349e655bd3e",
      "1e186ebb71274634a68c59db3eaae1ca",
      "f055bba7ffe940ec8e9304e11d5451f5",
      "68874d7d9f8047d5add35b886ba1e37e",
      "ead8ba6c8a2e4b75b4b3e650a2f648be",
      "3624888639cf4b01aace1ee7a7cfabe1",
      "b4b441e7a28346dd9b5ca740ee5af663",
      "cbbf4eee9b004457a4dc8690728d5849",
      "d184da6a93854f2abc3d50d454eb34b6",
      "faad89ce8d9949a9bab0788d327bf3b7",
      "86c571060a8347e88c327b27024c708b",
      "6d1e19027ac443988d6c59a6eac767b8",
      "bb7a8149837e4c518004de49a5bddb19",
      "ccf02cb0a18348e78efd232cc2441fc9",
      "46c87906e0e746f79b49750bab78c7ac",
      "52683de87f2e4436812dc5fab6f65fd5",
      "c12db02c685b42b7aa50e58017f3abb1",
      "11c98116ff15456ab274bccdbf540a69",
      "b022d36199b74a31ac3e064b4b20f315",
      "810241106eed45df94876e580ec7622e",
      "52edd70c934b49be80d6ad09d1d8d2f1",
      "a1d49828ec914b67ab3f96a08e0282fe",
      "bbaa37006fb645ee9393b65615a9c684",
      "d7d6a87717344feabb513e9238c1ba86",
      "4d80a4577e2f4bf48f0b85e84aeb5524",
      "4eada76c664847759f0afe7f4eac90d0",
      "f62cab51864a49119397ab1dd07c4326",
      "ad8914aed643452da9717a2e95208143",
      "71025b8d4def439b874b24cb81689c5b",
      "b4e72b7cc78e4fff81139fecb4dbde5e",
      "f6288dc2b8d74b41b7315d7e7dbe27a9",
      "61ddc3752ac54abb94aabe36f0e36222",
      "d877cd30ed154079ba43d1009a131f33",
      "37a77d1ffdd74c6fa1a5b01ec78d7ca6",
      "a88b88af615a4efb9f94668803a1332d",
      "103e95b4c0234d12983618db14d7e0e0",
      "154d96686b9e41078eed37ead12d265d",
      "89b0628eff644beaae373fd13a2ab41f",
      "d7974c6144a44002867ee9f86b8cca6a",
      "c5a39396b7b94851ba21ff4b9077da42",
      "bb97fa7f0d3b4b7aa47acf58b9679b64",
      "067d3883e67e40da9550dfcc12eccbf1",
      "854b0fd99f6d438fad7684e6e93fbb96",
      "cfe53b1103eb40dea1d73b6f5165d26a",
      "a0de5ce3806344c18ae088646aacabbb",
      "a9076342b011408684951389a59e80c7",
      "fce60b52115c4625ba8ad85002bec9ae",
      "d2fb594fd56348fb8c2d6cd6c2638856",
      "84f67b2288334ca787466585126a49fe",
      "1e285bb45cfa44ffb59c6af67189627a",
      "1b8ec7d57a1441ccb6dfa2f3ee2e81dd",
      "c48cb27f417a4d89b64916a126d6979d",
      "ed691208cc3749c29b0af31ed2782b5e",
      "2dfe125cb9254a9aaee3d768cd9b18dc",
      "68d2f31857cc45819891725a0e4dc5e5",
      "2ea189d7274643798e3530d64ba94cf5",
      "e321d6a4d06840ea8756c919e51f9d63",
      "bb6b554d404d4103911c4a34b15e9290",
      "13b492260139408a87007d45e1c0914b",
      "2189ebe7ba8f40ae9b62a1a6a35543a1",
      "10ab6cb949cd49c4b2367080b3105b9c",
      "013a241ce06a4d7080a2da0cffe1dafb",
      "fca0fe264d2348af8d96e36b3b95564f",
      "b8ed291efa71422bae6c3b654d06d002",
      "d8d8eec3aba04b7bbae98275905aaec0",
      "1850cdd2eb114ad7b66852dace2f2160",
      "55850e2aae2d4a1d9b9c8346983266ae",
      "16e2582812d44d2c813c8d1745afb056",
      "bf230616901846d8b0f8edcb8f7a7f20",
      "ade167a92bcb447489865ee41fc69022"
     ]
    },
    "id": "-CPgCL6AO8p8",
    "outputId": "c6f0b301-6271-4e04-9c67-05d1fee985c5"
   },
   "outputs": [],
   "source": [
    "output_dir = './models/llama3_8B/'\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "checkpoints = [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "latest_checkpoint = max(checkpoints, key=os.path.getctime) if checkpoints else None\n",
    "\n",
    "# Resume training from the latest checkpoint\n",
    "model_id = latest_checkpoint if latest_checkpoint else model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    print('Pad token is None')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fEhH8vhYyDLf"
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "l7E1a1uQymu-"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCFaTTxXlrWG"
   },
   "source": [
    "### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'preprocessed_output', 'summarized_input', 'summarized_output'],\n",
       "        num_rows: 89732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'preprocessed_output', 'summarized_input', 'summarized_output'],\n",
       "        num_rows: 11217\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input', 'preprocessed_output', 'summarized_input', 'summarized_output'],\n",
       "        num_rows: 11216\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HMCDataset = load_dataset('hakeematyab/HealthCareMagicWithSummary-100k')\n",
    "HMCDataset = HMCDataset['train'].train_test_split(train_size=0.8,seed=seed)\n",
    "HMCDatasetTemp = HMCDataset['test'].train_test_split(train_size=0.5,seed=seed)\n",
    "HMCDataset['validation'] = HMCDatasetTemp.pop('train')\n",
    "HMCDataset['test']  = HMCDatasetTemp.pop('test')\n",
    "del HMCDatasetTemp\n",
    "HMCDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples,teacher_forcing_ratio=1.0,accurate_context_ratio=0.5):    \n",
    "    prompt_template = '''### system: \n",
    "You are CareConnect, an expert medical personal assistant.\n",
    "\n",
    "### instruction: \n",
    "Answer the user's queries truthfully and accurately, based on the provided context if the context is applicable. Refuse to answer any questions unrelated to medicine.\n",
    "\n",
    "### context: \n",
    "{context}\n",
    "\n",
    "### user: \n",
    "{user_query}\n",
    "\n",
    "### system: \n",
    "{response}'''+tokenizer.eos_token\n",
    "    inputs = []\n",
    "    for user_query, output, summ_input, summ_output  in zip(examples['input'],examples['preprocessed_output'],examples['summarized_input'], examples['summarized_output']):\n",
    "        if random.random() < teacher_forcing_ratio:\n",
    "            if random.random() < accurate_context_ratio:\n",
    "                current_context= summ_input+'\\n'+summ_output\n",
    "            else:\n",
    "                wrong_input_context = random.choice([c for c in examples['summarized_input'] if c != summ_input])\n",
    "                wrong_output_context = random.choice([c for c in examples['summarized_output'] if c != summ_output])\n",
    "                current_context= wrong_input_context+'\\n'+wrong_output_context\n",
    "        else:\n",
    "            current_context= \"\"\n",
    "        inputs.append(prompt_template.format(context=current_context, user_query=user_query,response=output))\n",
    "    return {'text':inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'preprocessed_output', 'summarized_input', 'summarized_output', 'text'],\n",
       "        num_rows: 89732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'preprocessed_output', 'summarized_input', 'summarized_output', 'text'],\n",
       "        num_rows: 11217\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input', 'preprocessed_output', 'summarized_input', 'summarized_output', 'text'],\n",
       "        num_rows: 11216\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDataset=HMCDataset.map(preprocess_function, batched=True)\n",
    "fullDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### system: \n",
      "You are CareConnect, an expert medical personal assistant.\n",
      "\n",
      "### instruction: \n",
      "Answer the user's queries truthfully and accurately, based on the provided context if the context is applicable. Refuse to answer any questions unrelated to medicine.\n",
      "\n",
      "### context: \n",
      "I have had a bout of bronchitis. Now that I am over it, I can't do any work. Next week, I will have an echo cardiogram to check if I have a heart condition. I am 52 years old and 40 pounds overweight.\n",
      "According to the information provided by the doctor, low back pain is due to stenosis in the spine.\n",
      "\n",
      "### user: \n",
      "im a 39yr old female,i just had a spinal fusion surgery on my back in December. in December I missed my period,january comes I get my period but its really heavy and then I started vomiting for 8 days straight when my period was over s were my symptoms, then febuary comes same thing I get my period and vomit for 8 days again,now march just before my period started I vomited for 8 days then my period started and I vomited for another few days . ive been hospitalized for dehydration 5 times since jan. before my back surgery I was on a high dose of morphine for pain but im off it now and have been for 6 weeks my dr thought it could be my receptors in my brain he also said I didnt have any of my own endorphins to fight pain so I need to work on building them back up.but even now my dr is confused by my symtoms and is now sending me for a colonoscopy ive already had an ultrasound that came back normal and blood work came back good to except my white cell count has been concistantly high since this started I believe my cell count was 13,000 .  the er dr last night asked me if I use marijuana and I said I did for my pain cause I cant take pain killers so the er dr told me about a study being done called cannibanoid hyperemesis syndrome,never heard of it before but hes the only dr ive seen to mention it.\n",
      "\n",
      "### system: \n",
      "It is not that your spinal fusion got to do with pain, successful surgery will prevent all movement there and nerve impingement, so cause for pain is not it. Your doctor should gradually shift you on alternative painkillers if at all needed. Heavy periods and vomiting has got something to do with hormonal imbalance probably, and a gynecological consult is necessary. If USG is not finding anything, simple anti-emetics before periods must be taken and may be prescribed by genes specialist after giving you internal check and hormonal profile. If you are having abdominal pain, then maybe colonoscopy may be necessary for bowel symptoms/findings that has made your doctor think on those lines.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(fullDataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_6PmcYttoRFX"
   },
   "outputs": [],
   "source": [
    "data_collator= DataCollatorWithPadding(tokenizer=tokenizer,padding=\"max_length\", max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size=8\n",
    "per_device_eval_batch_size=8\n",
    "epochs = 1\n",
    "max_seq_length=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QHnNKrPqoX9O"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps= 16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=15,\n",
    "    save_steps=15,\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "    greater_is_better=False,  # Metric is minimized\n",
    "    logging_dir='./logs',\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=5,  # Log every 10 steps\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d789b0bbf3244f7e97127b2a8411de56",
      "4b089233adae4bc9b8235fad48cf0ba4",
      "808a53982ce94e51b11a9b785714eb60",
      "eb097f92be414d3195a03c3180dc3be9",
      "0a161a74bc304afd9d18102c04e48c10",
      "53751c2144bd4eff8b414e278e962c71",
      "75c0070618374cd29cd64dd30431855b",
      "b2a5dd36c80f41d1be717e7e1a2c77b4",
      "6bdf3aa9ee664aca888012b12280728b",
      "1290aef9b4444f4e8f680f475318538a",
      "528165cf387b4b6196ab76a97b5e1055"
     ]
    },
    "id": "_Ob8Ro8wdCT7",
    "outputId": "6f0b41e7-65b5-4c4f-c99d-69b055c346c6"
   },
   "outputs": [],
   "source": [
    "bert_score= evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, references = eval_preds\n",
    "    print(references.shape)\n",
    "    print(predictions[0].shape)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions[0], skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(references, skip_special_tokens=True)\n",
    "    print(decoded_preds)\n",
    "    print(decoded_labels)\n",
    "    result= bert_score.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    \n",
    "    return {'precision':np.mean(result['precision']),'recall':np.mean(result['recall']),'f1':np.mean(result['f1'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak. \n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    print(logits.shape)\n",
    "    print(labels.shape)\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "EOj2zZKp6_jK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You are adding a <class 'transformers.integrations.integration_utils.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "WandbCallback\n",
      "EarlyStoppingCallback\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field = \"text\",\n",
    "    train_dataset=fullDataset['train'],\n",
    "    eval_dataset=fullDataset['validation'],\n",
    "    args=training_args,\n",
    "    peft_config=config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3),WandbCallback(),ProgressCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.update(training_args.to_dict())\n",
    "\n",
    "# Convert trainer parameters to dictionary and log to W&B\n",
    "trainer_params = {\n",
    "    \"train_dataset_size\": len(trainer.train_dataset),\n",
    "    \"eval_dataset_size\": len(trainer.eval_dataset),\n",
    "    \"max_seq_length\": max_seq_length,\n",
    "    \"compute_metrics\":\"Bertscore\",\n",
    "    \"callbacks\": [\"Early Stopping: Patience=3\",\"Logging\",\"Wandb Reporting\"],\n",
    "}\n",
    "wandb.config.update(trainer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "U7KiOnGS7iW7"
   },
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python LLM-Finetuning",
   "language": "python",
   "name": "llm_finetuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
